{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0gUmi_rnPBa_"},"outputs":[],"source":["pip install datasets"]},{"cell_type":"markdown","metadata":{"id":"83RDehfpOATU"},"source":["#Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ecYjPzrPDOo"},"outputs":[],"source":["import os\n","import urllib\n","import io\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image as pimage\n","from transformers import CLIPModel, CLIPProcessor\n","from datasets import load_dataset\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"gd61DecONn2f"},"source":["# Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IuwEVNSSPJjm"},"outputs":[],"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 1024\n","LAION_SPLIT = \"train[:1%]\"  # Adjust to load a subset (1%)\n","\n","# --- Initialize CLIP Model ---\n","print(\"Loading CLIP model...\")\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"]},{"cell_type":"markdown","metadata":{"id":"2RclbL6zNueN"},"source":["# Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8fFupQ6OoMq"},"outputs":[],"source":["def download_image(url, timeout=5):\n","    \"\"\"\n","    Downloads an image from a URL with a custom User-Agent.\n","    Args:\n","        url: Image URL.\n","        timeout: Max time in seconds for download.\n","\n","    Returns:\n","        img_stream: BytesIO stream of the image.\n","    \"\"\"\n","    try:\n","        urllib_request = urllib.request.Request(\n","            url,\n","            data=None,\n","            headers={\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\"},\n","        )\n","        with urllib.request.urlopen(urllib_request, timeout=timeout) as r:\n","            img_stream = io.BytesIO(r.read())\n","        return img_stream\n","    except Exception as e:\n","        print(f\"Failed to download image from {url}: {e}\")\n","        return None\n","\n","def get_text_emb(text):\n","    \"\"\"\n","    Generates normalized text embeddings using CLIP.\n","    Args:\n","        text: Input text (string).\n","\n","    Returns:\n","        Normalized text embedding as a NumPy array.\n","    \"\"\"\n","    try:\n","        with torch.no_grad():\n","            inputs = processor(text=[text], return_tensors=\"pt\").to(DEVICE)\n","            text_emb = model.get_text_features(**inputs)\n","            text_emb /= text_emb.norm(dim=-1, keepdim=True)\n","            text_emb = text_emb.cpu().detach().numpy().astype(\"float32\")[0]\n","        return text_emb\n","    except Exception as e:\n","        print(f\"Error generating text embedding: {e}\")\n","        return None\n","\n","def get_image_emb(image_url):\n","    \"\"\"\n","    Generates normalized image embeddings using CLIP.\n","    Args:\n","        image_url: URL of the image.\n","\n","    Returns:\n","        Normalized image embedding as a NumPy array.\n","    \"\"\"\n","    try:\n","        img_stream = download_image(image_url)\n","        if img_stream is None:\n","            raise Exception(\"Image download failed.\")\n","\n","        image = pimage.open(img_stream).convert(\"RGB\")\n","        with torch.no_grad():\n","            inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n","            image_emb = model.get_image_features(**inputs)\n","            image_emb /= image_emb.norm(dim=-1, keepdim=True)\n","            image_emb = image_emb.cpu().detach().numpy().astype(\"float32\")[0]\n","        return image_emb\n","    except Exception as e:\n","        return None\n","\n","# --- Dataset Class ---\n","class LAIONDataset(Dataset):\n","    \"\"\"\n","    Custom dataset for LAION data containing image URLs and captions.\n","    \"\"\"\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        sample = self.dataset[idx]\n","        image_url = sample[\"image_url\"]\n","        caption = sample[\"caption\"]\n","        return image_url, caption"]},{"cell_type":"markdown","metadata":{"id":"atDn1Tf3N2yL"},"source":["# Load LAION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wrkisoMlPSYM"},"outputs":[],"source":["print(\"Loading LAION dataset...\")\n","laion_dataset = load_dataset(\"conceptual_captions\", split=LAION_SPLIT)\n","laion_train_dataset = LAIONDataset(laion_dataset)\n","laion_dataloader = DataLoader(laion_train_dataset, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"xrQNbv4dNzxC"},"source":["# Extract Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHfgyypNO18L"},"outputs":[],"source":["image_embeddings = []\n","text_embeddings = []\n","\n","def process_single_example(image_url, caption):\n","    \"\"\"\n","    Process a single image and caption pair to extract embeddings.\n","    Returns:\n","        (image_emb, text_emb) tuple or (None, None) if image fails.\n","    \"\"\"\n","    try:\n","        image_emb = get_image_emb(image_url)\n","        if image_emb is not None:\n","            text_emb = get_text_emb(caption)\n","            return image_emb, text_emb\n","        return None, None\n","    except Exception as e:\n","        print(f\"Error processing {image_url}: {e}\")\n","        return None, None\n","\n","print(\"Extracting embeddings...\")\n","with ThreadPoolExecutor(max_workers=8) as executor:  # Adjust max_workers based on CPU cores\n","    for batch_idx, (image_urls, captions) in enumerate(tqdm(laion_dataloader, desc=\"Processing Batches\")):\n","        batch_image_embeds = []\n","        batch_text_embeds = []\n","        futures = []\n","\n","        # Submit tasks to the ThreadPoolExecutor\n","        for image_url, caption in zip(image_urls, captions):\n","            futures.append(executor.submit(process_single_example, image_url, caption))\n","\n","        # Collect results as they complete\n","        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Batch {batch_idx + 1}\", leave=False):\n","            image_emb, text_emb = future.result()\n","            if image_emb is not None and text_emb is not None:\n","                batch_image_embeds.append(image_emb)\n","                batch_text_embeds.append(text_emb)\n","\n","        # Append valid embeddings\n","        image_embeddings.extend(batch_image_embeds)\n","        text_embeddings.extend(batch_text_embeds)\n","\n","        print(f\"Batch {batch_idx + 1}: Processed {len(batch_image_embeds)} images and {len(batch_text_embeds)} captions\")\n","\n","# Convert embeddings to NumPy arrays\n","image_embeddings = np.array(image_embeddings)\n","text_embeddings = np.array(text_embeddings)\n","\n","# Optional: Save embeddings to disk\n","# np.save(\"image_embeddings.npy\", image_embeddings)\n","# np.save(\"text_embeddings.npy\", text_embeddings)\n","\n","print(f\"Extracted {len(image_embeddings)} image embeddings and {len(text_embeddings)} text embeddings.\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}