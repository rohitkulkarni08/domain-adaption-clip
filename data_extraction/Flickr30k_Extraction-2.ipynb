{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"8siuR4yHvCEs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734754752074,"user_tz":300,"elapsed":32249,"user":{"displayName":"Mayukh Sen","userId":"13067369494780001387"}},"outputId":"896d4584-069f-4158-d33e-31fb7fb94248"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9mPU2eluK-6"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from PIL import Image\n","from transformers import CLIPProcessor, CLIPModel, AutoTokenizer\n","from collections import defaultdict\n","\n","# Paths\n","FLICKR_IMAGE_DIR = \"/content/drive/MyDrive/Stat_Learning_Project/Flickr30K/images/flickr30k-images\"\n","FLICKR_TOKEN_FILE = \"/content/drive/MyDrive/Stat_Learning_Project/Flickr30K/results_20130124.token\"\n","\n","OUTPUT_IMAGE_EMBEDDINGS = \"/content/drive/MyDrive/Stat_Learning_Project/Flickr30K/flickr_image_embeddings.npy\"\n","OUTPUT_CAPTIONS = \"/content/drive/MyDrive/Stat_Learning_Project/Flickr30K/flickr_captions.npy\"\n","\n","# Initialize CLIP model and processor\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","source":["from tqdm import tqdm\n","from concurrent.futures import ThreadPoolExecutor"],"metadata":{"id":"xKZPeqrKymUE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","TAR_FILE_PATH = \"/content/drive/MyDrive/Stat_Learning_Project/Flickr30K/flickr30k-images.tar\"\n","\n","EXTRACTION_DIR = \"/content/drive/MyDrive/Stat_Learning_Project/Flickr30K/images/flickr30k-images\"\n","\n","# Extract if not already extracted\n","if not os.path.exists(EXTRACTION_DIR):\n","    os.makedirs(EXTRACTION_DIR)\n","    with tarfile.open(TAR_FILE_PATH, 'r') as tar:\n","        tar.extractall(path=EXTRACTION_DIR)\n","        print(f\"Extracted images to {EXTRACTION_DIR}\")\n","else:\n","    print(f\"Images already extracted in {EXTRACTION_DIR}\")"],"metadata":{"id":"HxHCh-lpBX1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_flickr30k(token_file_path):\n","    captions_dict = defaultdict(list)\n","    with open(token_file_path, \"r\") as file:\n","        for line in file:\n","            caption_id, caption = line.strip().split(\"\\t\")\n","            image_id = caption_id.split(\"#\")[0]\n","            captions_dict[image_id].append(caption)\n","    return captions_dict\n","\n","def load_image_and_caption(image_dir, image_id, caption):\n","    try:\n","        image_path = os.path.join(image_dir, image_id)\n","        image = Image.open(image_path).convert(\"RGB\")\n","        return image, caption\n","    except Exception as e:\n","        print(f\"Error loading {image_id}: {e}\")\n","        return None, None\n","\n","def convert_flickr_to_npy(image_dir, captions_dict, output_image_path, output_caption_path, batch_size=32):\n","    image_embeddings = []\n","    tokenized_captions = []\n","\n","    image_ids = list(captions_dict.keys())\n","    all_captions = [captions_dict[image_id][0] for image_id in image_ids]\n","\n","    loaded_images = [None] * len(image_ids)\n","    loaded_captions = [None] * len(image_ids)\n","\n","    print(\"Loading images and captions in parallel...\")\n","    with ThreadPoolExecutor() as executor:\n","        futures = {\n","            executor.submit(load_image_and_caption, image_dir, image_ids[i], all_captions[i]): i\n","            for i in range(len(image_ids))\n","        }\n","        for future in tqdm(futures, total=len(futures), desc=\"Loading\"):\n","            index = futures[future]\n","            image, caption = future.result()\n","            if image and caption:\n","                loaded_images[index] = image\n","                loaded_captions[index] = caption\n","\n","    valid_indices = [i for i in range(len(loaded_images)) if loaded_images[i] is not None]\n","    loaded_images = [loaded_images[i] for i in valid_indices]\n","    loaded_captions = [loaded_captions[i] for i in valid_indices]\n","\n","    print(\"Processing images and captions in batches...\")\n","    for i in tqdm(range(0, len(loaded_images), batch_size), desc=\"Batch Processing\"):\n","        batch_images = loaded_images[i:i+batch_size]\n","        batch_captions = loaded_captions[i:i+batch_size]\n","\n","        inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True).to(DEVICE)\n","        with torch.no_grad():\n","            batch_image_emb = clip_model.get_image_features(**inputs)\n","            batch_image_emb /= batch_image_emb.norm(dim=-1, keepdim=True)\n","        image_embeddings.append(batch_image_emb.cpu().numpy())\n","\n","        tokens = tokenizer(batch_captions, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n","        tokenized_captions.append(tokens.input_ids.numpy())\n","\n","    np.save(output_image_path, np.vstack(image_embeddings))\n","    np.save(output_caption_path, np.vstack(tokenized_captions))"],"metadata":{"id":"LocgiSxfufG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run the conversion\n","print(\"Parsing captions...\")\n","captions_dict = parse_flickr30k(FLICKR_TOKEN_FILE)\n","\n","print(\"Processing images and captions...\")\n","convert_flickr_to_npy(FLICKR_IMAGE_DIR, captions_dict, OUTPUT_IMAGE_EMBEDDINGS, OUTPUT_CAPTIONS)"],"metadata":{"id":"FTHZi9w5udfu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["caption_embeddings = np.load(FLICKR_CAPTIONS)\n","\n","caption_embeddings_norm = caption_embeddings / np.linalg.norm(caption_embeddings, axis=-1, keepdims=True)\n","\n","np.save('/content/drive/MyDrive/Stat_Learning_Project/Flickr30K/images/Flickr30k/flickr_captions_normalized.npy', caption_embeddings_norm.astype(np.float32))\n","\n","print(\"Flickr30k conversion complete!\")"],"metadata":{"id":"TFNCwNFGu_oa"},"execution_count":null,"outputs":[]}]}